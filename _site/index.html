<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

<!-- Begin Jekyll SEO tag v2.3.0 -->
<title>EASE Lab Explorations with Azure Kinect | step-kinect-dev</title>
<meta property="og:title" content="EASE Lab Explorations with Azure Kinect" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="step-kinect-dev" />
<script type="application/ld+json">
{"name":"step-kinect-dev","description":null,"author":null,"@type":"WebSite","url":"http://localhost:4000/","publisher":null,"image":null,"headline":"EASE Lab Explorations with Azure Kinect","dateModified":null,"datePublished":null,"sameAs":null,"mainEntityOfPage":null,"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=572d5c883111d73f6bd9309a3ef4fd65079913bc">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>step-kinect-dev</h1>
        <p></p>

        
          <p class="view"><a href="http://github.com/amonmillner/step-kinect-dev">View the Project on GitHub <small></small></a></p>
        

        

        
      </header>
      <section>

      <h2 id="ease-lab-explorations-with-azure-kinect">EASE Lab Explorations with Azure Kinect</h2>

<p>Dr. Amon Millner, Hwei-Shin Harriman, Richard Gao of the Olin College of Engineering Extending Access to STEM Empowerment (EASE) Lab are engaging with the Microsoft Azure Kinect Development Kit to leverage its unique features in a context related to step performances.</p>

<p>Through this page, we are sharing early experiments in adding new dimensions to step performances.</p>

<p>Use this map to the page to follow our progress.</p>

<h3 id="the-inspiration">The Inspiration</h3>
<ol>
  <li><a href="#movie">Seeing a Step Show on stage and on screen</a></li>
  <li><a href="#usc">Stepping on campus</a></li>
  <li><a href="#nmaahc">Experiencing steps + Kinect tech + kids</a></li>
</ol>

<h3 id="the-vision">The Vision</h3>
<ol>
  <li><a href="#ease">Relating to EASE Lab work</a></li>
</ol>

<h3 id="the-technical-exploration-sprint">The Technical Exploration Sprint</h3>
<ol>
  <li><a href="#stepkinect">Adding Azure services to Kinect-enhanced steps</a>
    <ul>
      <li><a href="#stepsense">sensing steps with cameras</a></li>
      <li><a href="#face">recognizing emotion with azure</a></li>
      <li><a href="#viz">making responsive graphic visualizations</a></li>
      <li><a href="#sounds">generating sounds with steps</a></li>
    </ul>
  </li>
</ol>

<h3 id="future-directions">Future Directions</h3>
<ol>
  <li><a href="#collabo">Seeding collaborations</a></li>
</ol>

<p><a name="movie"></a></p>
<h2 id="the-inspiration-1">The Inspiration</h2>
<h3 id="seeing-a-step-show-on-stage-and-on-screen">Seeing a Step Show on stage and on screen</h3>

<p>As a middle schooler, Amon was intrigued by a few aspects of college, the computer science major and step shows. He saw college through the lens of a family friend, who invited him to watch her step show. He experience the electric atmosphere of the synchronized sound in person and later enjoyed how step shows were portrayed as a symbol of simultaneous brotherhood/sisterhood and sibling competition at the same time - such as the following clip from the 1980’s “School Daze” film:</p>

<iframe width="640" height="480" src="https://www.youtube.com/embed/WfPc8Ia7E20?start=38" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<p>”&gt;</p>

<p><a name="usc"></a></p>

<h3 id="stepping-on-campus">Stepping on campus</h3>

<p>As a computer science major in college, Amon also participated in step shows.</p>

<p><img src="/photos/usc1.jpg" alt="" /></p>

<p>Amon is also old, so we have not yet transferred VHS footage of him stepping into digital images so the placeholder above will have to do.</p>

<p><a name="nmaahc"></a></p>

<h2 id="experiencing-steps--kinect-tech--kids">Experiencing Steps + Kinect tech + kids</h2>

<p>Amon has had a close encounter with an interactive stepping experience that made use of older Kinect sensors. See his kids at the National Museum of African American History and Culture following the lead of on-screen step instructors while they see point cloud-esque likenesses of themselves on the screen alongside the virtual performers.</p>

<p><img src="/photos/nmaahc0.jpg" alt="" /></p>

<iframe src="https://drive.google.com/file/d/197w6iOkJ9QfrqRP9aMSpM0E64h61uHtl/preview" width="640" height="480"></iframe>

<p>Observing how engaging the exhibit was made Amon leave wondering what other types of interactions could be supported with technologies such as Kinects.</p>

<p>Amon was excited to get his hands on the Azure Kinect DK, because he could engage students in his Lab in a very ambitious exploration that connects to his heritage. The art of stepping has a rich past with roots in gumboot dancing, shown in this image from the NMAAHC museum. The art also has great potential to evolve in ways that leverage technological advances. This exploration is an early attempt to experiment in the space of stepping.</p>

<p><a name="ease"></a></p>

<h2 id="the-vision-1">The Vision</h2>

<h3 id="relating-to-ease-lab-work">Relating to EASE Lab work</h3>

<p>I’ll explain here how this work can exhibit cultures that are needing more representation at the forefront of advanced technologies.</p>

<p>Making interests come to life in new ways through computing.
Our team has experience with culturally-relevant performances and how they intersect with code.</p>

<h3 id="the-vision-in-full">The Vision in full</h3>

<p>People would…
and they would see code</p>

<p>image of the snake face</p>

<p><a name="stepkinect"></a></p>

<h2 id="the-technical-exploration-sprint-1">The Technical Exploration Sprint</h2>

<h3 id="adding-azure-to-kinect-enhanced-steps-in-a-sprint">Adding Azure to Kinect-enhanced steps in a sprint</h3>

<p>Our first sprint goal was to integrate different aspects of the Azure Kinect’s features that have not been available before in previous Kinect models while introducing compelling contexts.</p>

<p><a href="">the MS Azure Kinect SDK site</a> shows the full list of the device’s features. Our initial efforts explored only a subset: video sensing and Azure Cognitive Services.</p>

<p>We have developed a system to use the Kinect camera to track the points on a person’s leg to determine when a “stomp” is happening or when hands are clapping. While people are (hopefully stepping) in front of the camera, we send full frame images to Cognitive Services face API to receive the likely emotion expressed during the frame.</p>

<p>We elected to use openFrameworks extensively in the first sprint. This gave us the ability to explore making every stomp that is registered trigger a graphical visualization under the stepper’s foot. We also play back different sounds for stomps than we do for claps.</p>

<p>We change the background of the scene to red when the Face API informs the program that the stepper is considered to be showing angered face. When happiness is detected, the background is light gold.</p>

<p><a name="stepsense"></a></p>

<h4 id="sensing-steps-with-cameras">Sensing steps with cameras</h4>

<p>The ver 0.1 code snippet at the bottom of this subsection gives an example of how we went about detecting when a stepper stomps the ground. The approach is reasonable albeit not super robust at the moment. To get the proof of concept working, we needed to expose timestamp information from the openFrameworks codebase by adding functions to retrieve the private timestamp variable, so that we could determine DP/DT of two samples - only using the Y. We did not use pure distance formula because we were only concerned with a leg going up or down, which the Kinect maps to the Y coordinate. The program considers a stomp to occur When the current velocity is small, but the preceding velocity was large.</p>

<p>We had to modify source files from the openFrameworks Kinect add-on to expose the timestamps.</p>

<p>code is from the ofapp.cpp</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//TODO: Refactor into MOVE Detection Class
</span><span class="kt">double</span> <span class="n">ofApp</span><span class="o">::</span><span class="n">calculateDistance</span><span class="p">(</span><span class="n">k4a_float3_t</span> <span class="n">pos1</span><span class="p">,</span> <span class="n">k4a_float3_t</span> <span class="n">pos2</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">auto</span> <span class="n">xDistance</span> <span class="o">=</span> <span class="n">pow</span><span class="p">(</span><span class="n">pos2</span><span class="p">.</span><span class="n">xyz</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">pos1</span><span class="p">.</span><span class="n">xyz</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
	<span class="k">auto</span> <span class="n">yDistance</span> <span class="o">=</span> <span class="n">pow</span><span class="p">(</span><span class="n">pos2</span><span class="p">.</span><span class="n">xyz</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="n">pos1</span><span class="p">.</span><span class="n">xyz</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
	<span class="k">auto</span> <span class="n">zDistance</span> <span class="o">=</span> <span class="n">pow</span><span class="p">(</span><span class="n">pos2</span><span class="p">.</span><span class="n">xyz</span><span class="p">.</span><span class="n">z</span> <span class="o">-</span> <span class="n">pos1</span><span class="p">.</span><span class="n">xyz</span><span class="p">.</span><span class="n">z</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
	<span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">xDistance</span> <span class="o">+</span> <span class="n">yDistance</span> <span class="o">+</span> <span class="n">zDistance</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">double</span> <span class="n">ofApp</span><span class="o">::</span><span class="n">calculateYDist</span><span class="p">(</span><span class="n">k4a_float3_t</span> <span class="n">pos1</span><span class="p">,</span> <span class="n">k4a_float3_t</span> <span class="n">pos2</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">double</span> <span class="n">dp</span> <span class="o">=</span> <span class="n">pos2</span><span class="p">.</span><span class="n">xyz</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="n">pos1</span><span class="p">.</span><span class="n">xyz</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">abs</span><span class="p">(</span><span class="n">dp</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// TODO: Refactor into Stomp/Clap Detection class
</span><span class="kt">void</span> <span class="n">ofApp</span><span class="o">::</span><span class="n">saveJointsAndVel</span><span class="p">(</span><span class="k">const</span> <span class="n">k4abt_joint_t</span> <span class="n">joints</span><span class="p">[</span><span class="mi">32</span><span class="p">],</span>
	<span class="kt">double</span> <span class="n">leftVel</span><span class="p">,</span>
	<span class="kt">double</span> <span class="n">rightVel</span><span class="p">,</span>
	<span class="kt">uint64_t</span> <span class="n">timeStamp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="c1">//STOMP DETECTION
</span>	<span class="n">m_leftAnkle</span> <span class="o">=</span> <span class="n">joints</span><span class="p">[</span><span class="n">K4ABT_JOINT_ANKLE_LEFT</span><span class="p">].</span><span class="n">position</span><span class="p">;</span>
	<span class="n">m_rightAnkle</span> <span class="o">=</span> <span class="n">joints</span><span class="p">[</span><span class="n">K4ABT_JOINT_ANKLE_RIGHT</span><span class="p">].</span><span class="n">position</span><span class="p">;</span>
	<span class="n">m_lastLeftVel</span> <span class="o">=</span> <span class="n">leftVel</span><span class="p">;</span>
	<span class="n">m_lastRightVel</span> <span class="o">=</span> <span class="n">rightVel</span><span class="p">;</span>
	<span class="n">m_lastTimeStamp</span> <span class="o">=</span> <span class="n">timeStamp</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">//TODO: Refactor into Stomp Detection class
</span><span class="kt">double</span> <span class="n">ofApp</span><span class="o">::</span><span class="n">calculateVelocity</span><span class="p">(</span><span class="k">const</span> <span class="n">k4abt_joint_t</span> <span class="n">joints</span><span class="p">[</span><span class="mi">32</span><span class="p">],</span> <span class="kt">int</span> <span class="n">leg</span><span class="p">,</span> <span class="kt">uint64_t</span> <span class="n">timeStamp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="c1">//KNEE_LEFT 19, ANKLE_LEFT 20, KNEE_RIGHT 23, ANKLE_RIGHT 24
</span>	<span class="kt">double</span> <span class="n">dp</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">leg</span> <span class="o">==</span> <span class="n">LEFT</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">dp</span> <span class="o">=</span> <span class="n">calculateYDist</span><span class="p">(</span><span class="n">joints</span><span class="p">[</span><span class="n">K4ABT_JOINT_ANKLE_LEFT</span><span class="p">].</span><span class="n">position</span><span class="p">,</span> <span class="n">m_leftAnkle</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">leg</span> <span class="o">==</span> <span class="n">RIGHT</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">dp</span> <span class="o">=</span> <span class="n">calculateYDist</span><span class="p">(</span><span class="n">joints</span><span class="p">[</span><span class="n">K4ABT_JOINT_ANKLE_RIGHT</span><span class="p">].</span><span class="n">position</span><span class="p">,</span> <span class="n">m_rightAnkle</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="kt">double</span> <span class="n">dt</span> <span class="o">=</span> <span class="kt">double</span><span class="p">(</span><span class="n">timeStamp</span> <span class="o">-</span> <span class="n">m_lastTimeStamp</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1000000</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">dp</span> <span class="o">/</span> <span class="n">dt</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">//TODO: Refactor into Stomp Detection Class
</span><span class="kt">int</span> <span class="n">ofApp</span><span class="o">::</span><span class="n">didStomp</span><span class="p">(</span><span class="kt">double</span> <span class="n">velocity</span><span class="p">,</span> <span class="kt">double</span> <span class="n">lastVelocity</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">velocity</span> <span class="o">&lt;=</span> <span class="mf">700.0</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">lastVelocity</span> <span class="o">&gt;</span> <span class="mf">1500.0</span><span class="p">)</span> <span class="p">{</span>
			<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
		<span class="p">}</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a name="face"></a></p>
<h4 id="recognizing-emotion-with-azure">Recognizing emotion with azure</h4>

<p>To detect emotions of a person in front of the Kinect. The code below shows
how we grab an image which came from a Kinect for Azure format. We used the SDK’s function to access the image buffer (a pointer to the RAW image data). We decoded the buffer to convert the raw buffer to a JPEG format using OpenCV.</p>

<p>We used the MS CPP REST SDK that has been around for a while to send image to the Cognitive Services Face API. We sent the binary file input to get back one of the 8 emotions that the Face API classifies (with % of confidence). We had to open the istream with the proper formatting to avoid the Face API failing to return an emotion with no error. We initially did not have the proper format for the anonymous function and got hung there for a while.</p>

<p><a name="viz"></a></p>
<h4 id="making-responsive-graphic-visualizations">Making responsive graphic visualizations</h4>

<p>The background color of the scene changes when Face API returns an emotion. We did not use all of the classification possible: Anger, contempt, disgust, fear, happiness, neutral, sadness, or surprise.</p>

<p>This code sets the background color.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kt">bool</span> <span class="n">Device</span><span class="o">::</span><span class="n">updateColorInDepthFrame</span><span class="p">(</span><span class="k">const</span> <span class="n">k4a</span><span class="o">::</span><span class="n">image</span><span class="o">&amp;</span> <span class="n">depthImg</span><span class="p">,</span> <span class="k">const</span> <span class="n">k4a</span><span class="o">::</span><span class="n">image</span><span class="o">&amp;</span> <span class="n">colorImg</span><span class="p">)</span>
 <span class="p">{</span>
   <span class="k">const</span> <span class="k">auto</span> <span class="n">depthDims</span> <span class="o">=</span> <span class="n">glm</span><span class="o">::</span><span class="n">ivec2</span><span class="p">(</span><span class="n">depthImg</span><span class="p">.</span><span class="n">get_width_pixels</span><span class="p">(),</span> <span class="n">depthImg</span><span class="p">.</span><span class="n">get_height_pixels</span><span class="p">());</span>

   <span class="n">k4a</span><span class="o">::</span><span class="n">image</span> <span class="n">transformedColorImg</span><span class="p">;</span>
   <span class="k">try</span>
   <span class="p">{</span>
     <span class="n">transformedColorImg</span> <span class="o">=</span> <span class="n">k4a</span><span class="o">::</span><span class="n">image</span><span class="o">::</span><span class="n">create</span><span class="p">(</span><span class="n">K4A_IMAGE_FORMAT_COLOR_BGRA32</span><span class="p">,</span>
       <span class="n">depthDims</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">depthDims</span><span class="p">.</span><span class="n">y</span><span class="p">,</span>
       <span class="n">depthDims</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">uint8_t</span><span class="p">)));</span>

     <span class="k">this</span><span class="o">-&gt;</span><span class="n">transformation</span><span class="p">.</span><span class="n">color_image_to_depth_camera</span><span class="p">(</span><span class="n">depthImg</span><span class="p">,</span> <span class="n">colorImg</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">transformedColorImg</span><span class="p">);</span>
   <span class="p">}</span>
   <span class="k">catch</span> <span class="p">(</span><span class="k">const</span> <span class="n">k4a</span><span class="o">::</span><span class="n">error</span><span class="o">&amp;</span> <span class="n">e</span><span class="p">)</span>
   <span class="p">{</span>
     <span class="n">ofLogError</span><span class="p">(</span><span class="n">__FUNCTION__</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">e</span><span class="p">.</span><span class="n">what</span><span class="p">();</span>
     <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
   <span class="p">}</span>

   <span class="k">const</span> <span class="k">auto</span> <span class="n">transformedColorData</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">transformedColorImg</span><span class="p">.</span><span class="n">get_buffer</span><span class="p">());</span>

   <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">colorInDepthPix</span><span class="p">.</span><span class="n">isAllocated</span><span class="p">())</span>
   <span class="p">{</span>
     <span class="k">this</span><span class="o">-&gt;</span><span class="n">colorInDepthPix</span><span class="p">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">depthDims</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">depthDims</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">OF_PIXELS_BGRA</span><span class="p">);</span>
     <span class="k">this</span><span class="o">-&gt;</span><span class="n">colorInDepthTex</span><span class="p">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">depthDims</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">depthDims</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">GL_RGBA8</span><span class="p">,</span> <span class="n">ofGetUsingArbTex</span><span class="p">(),</span> <span class="n">GL_BGRA</span><span class="p">,</span> <span class="n">GL_UNSIGNED_BYTE</span><span class="p">);</span>
     <span class="k">this</span><span class="o">-&gt;</span><span class="n">colorInDepthTex</span><span class="p">.</span><span class="n">setTextureMinMagFilter</span><span class="p">(</span><span class="n">GL_NEAREST</span><span class="p">,</span> <span class="n">GL_NEAREST</span><span class="p">);</span>
     <span class="k">this</span><span class="o">-&gt;</span><span class="n">colorInDepthTex</span><span class="p">.</span><span class="n">bind</span><span class="p">();</span>
     <span class="p">{</span>
       <span class="n">glTexParameteri</span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">colorInDepthTex</span><span class="p">.</span><span class="n">texData</span><span class="p">.</span><span class="n">textureTarget</span><span class="p">,</span> <span class="n">GL_TEXTURE_SWIZZLE_R</span><span class="p">,</span> <span class="n">GL_BLUE</span><span class="p">);</span>
       <span class="n">glTexParameteri</span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">colorInDepthTex</span><span class="p">.</span><span class="n">texData</span><span class="p">.</span><span class="n">textureTarget</span><span class="p">,</span> <span class="n">GL_TEXTURE_SWIZZLE_B</span><span class="p">,</span> <span class="n">GL_RED</span><span class="p">);</span>
     <span class="p">}</span>
     <span class="k">this</span><span class="o">-&gt;</span><span class="n">colorInDepthTex</span><span class="p">.</span><span class="n">unbind</span><span class="p">();</span>
   <span class="p">}</span>

   <span class="k">this</span><span class="o">-&gt;</span><span class="n">colorInDepthPix</span><span class="p">.</span><span class="n">setFromPixels</span><span class="p">(</span><span class="n">transformedColorData</span><span class="p">,</span> <span class="n">depthDims</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">depthDims</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span>
   <span class="k">this</span><span class="o">-&gt;</span><span class="n">colorInDepthTex</span><span class="p">.</span><span class="n">loadData</span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">colorInDepthPix</span><span class="p">);</span>

   <span class="n">ofLogVerbose</span><span class="p">(</span><span class="n">__FUNCTION__</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">"Color in Depth "</span> <span class="o">&lt;&lt;</span> <span class="n">depthDims</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;&lt;</span> <span class="s">"x"</span> <span class="o">&lt;&lt;</span> <span class="n">depthDims</span><span class="p">.</span><span class="n">y</span> <span class="o">&lt;&lt;</span> <span class="s">" stride: "</span> <span class="o">&lt;&lt;</span> <span class="n">transformedColorImg</span><span class="p">.</span><span class="n">get_stride_bytes</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">"."</span><span class="p">;</span>

   <span class="n">transformedColorImg</span><span class="p">.</span><span class="n">reset</span><span class="p">();</span>

   <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
 <span class="p">}</span>
</code></pre></div></div>

<p>Underfoot animations.</p>

<p>We drew up a design for creating an effect that triggers when a stomp happens and briefly gives particle effect graphical feedback. The program can inform how mild or wild the effects are expressed based on the speed of the stomp or the last read emotion of the stepper.</p>

<p>The initial approach to emitting graphics involves importing an object such as a sphere and use its one coordinate to guide its path. The number of spheres, their appearance, and their paths are works-in-progress. openFrameworks appears to have a myriad of options for importing objects.</p>

<p><a name="sounds"></a></p>
<h4 id="generating-sounds-with-steps">Generating sounds with steps</h4>

<p>openFrameworks features a straightforward method for playing sound. We currently have short mp3 files of sound effects, one specifically to play when a stomp event is registered, a different one for when a clap event is registered. Currently, we are fine tuning appropriate sound effects and acceptable latency so that the sounds seem responsive to their triggers. We see the soundscape as something that change according to emotion as well.</p>

<p><a name="collabo"></a></p>
<h2 id="future-directions-1">Future Directions</h2>

<p>Once we have results that we’re happy with for one stepper in front of the camera, we will look to sense multiple bodies in the frame. The Cognitive Services Face API returns the bounding box attributes of the face it is classifying an emotion for, which will give us an ability to change a particular stepper’s outline or body color instead of the entire scene, for more interactions - can your team take on different personalities or stay in an intense anger face for the entire show (not an easy feat).</p>

<p>We expect to give the microphone array some added functionality through code. We can filter some of the input.</p>

<p>We learned that getting Visual Studio running can take time.
getting environment variables and linkers all working didn’t come out of the box. Some steps are not documented. When you open a solution intitially, it defaults to a 32-bit debugger and a 64-bit debugger is needed and the errors do not inform you very well.
Going through solution property pages to add additional include directories such azure kinect and body tracking libraries. The sentence that reminds users to “make sure that you remember to include such and such dll from this bin and library” without saying how to do that and make sure that you have it.
The whole of visual studio works well when a solution file can be opened.</p>

<h3 id="seeding-collaborations">Seeding collaborations</h3>

<p>Should this work continue…
A musician could augment these efforts in the following ways…</p>


      </section>
      <footer>
        
        <p>This project is maintained by <a href="http://github.com/amonmillner">amonmillner</a></p>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>


  
  </body>
</html>
